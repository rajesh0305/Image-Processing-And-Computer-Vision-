{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajesh0305/Image-Processing-And-Computer-Vision-/blob/main/AlexNet_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekVzSW8Bb_Ze",
        "outputId": "512ccc89-a31a-410a-e2af-48256dfd4c71"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "import wandb\n",
        "import os"
      ],
      "metadata": {
        "id": "HUjjD6MRYJmr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# STEP 0: Initialize wandb\n",
        "# =======================\n",
        "wandb.init(project=\"alexnet-flowers-v2\", config={\n",
        "    \"epochs\": 20,\n",
        "    \"batch_size\": 16,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"architecture\": \"alexnet\",\n",
        "    \"pretrained\": True,\n",
        "    \"input_size\": 128\n",
        "})\n",
        "\n",
        "# Shortcut to config values\n",
        "config = wandb.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "zxVl69yfYLO3",
        "outputId": "c1fdf72f-5c5a-4706-a821-789b481c1b66"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇▇▇█▇█</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▂▁</td></tr><tr><td>val_accuracy</td><td>▂▁▆▆▅█▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_accuracy</td><td>0.9229</td></tr><tr><td>train_loss</td><td>56.29839</td></tr><tr><td>val_accuracy</td><td>0.88526</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">whole-bird-2</strong> at: <a href='https://wandb.ai/rajesh8368568776-national-institute-of-technology-karnat/alexnet-flowers-v2/runs/cn904o4q' target=\"_blank\">https://wandb.ai/rajesh8368568776-national-institute-of-technology-karnat/alexnet-flowers-v2/runs/cn904o4q</a><br> View project at: <a href='https://wandb.ai/rajesh8368568776-national-institute-of-technology-karnat/alexnet-flowers-v2' target=\"_blank\">https://wandb.ai/rajesh8368568776-national-institute-of-technology-karnat/alexnet-flowers-v2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251210_065706-cn904o4q/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251210_080931-jw0kso6j</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rajesh8368568776-national-institute-of-technology-karnat/alexnet-flowers-v2/runs/jw0kso6j' target=\"_blank\">good-dawn-3</a></strong> to <a href='https://wandb.ai/rajesh8368568776-national-institute-of-technology-karnat/alexnet-flowers-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/rajesh8368568776-national-institute-of-technology-karnat/alexnet-flowers-v2' target=\"_blank\">https://wandb.ai/rajesh8368568776-national-institute-of-technology-karnat/alexnet-flowers-v2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/rajesh8368568776-national-institute-of-technology-karnat/alexnet-flowers-v2/runs/jw0kso6j' target=\"_blank\">https://wandb.ai/rajesh8368568776-national-institute-of-technology-karnat/alexnet-flowers-v2/runs/jw0kso6j</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# STEP 1: Data Preparation\n",
        "# =======================\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((config.input_size, config.input_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dir = \"/content/drive/MyDrive/5flowersdata-20251210T063634Z-3-001/5flowersdata/flowers/train\"\n",
        "val_dir = \"/content/drive/MyDrive/5flowersdata-20251210T063634Z-3-001/5flowersdata/flowers/val\"\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config.batch_size)"
      ],
      "metadata": {
        "id": "Ti4rs4tkYMos"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# STEP 2: Load Pretrained Model\n",
        "# ===========================\n",
        "\n",
        "alexnet = models.alexnet(pretrained=config.pretrained)\n",
        "# alexnet.classifier[6] = nn.Linear(4096, 5)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in alexnet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the final classifier layer (trainable)\n",
        "alexnet.classifier[6] = nn.Linear(4096, 5)\n",
        "\n",
        "# Only the new final layer should be trainable\n",
        "for param in alexnet.classifier[6].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "alexnet = alexnet.to(device)\n",
        "\n",
        "# Watch the model's weights and gradients\n",
        "wandb.watch(alexnet, log=\"all\", log_freq=10)"
      ],
      "metadata": {
        "id": "Uu1_CDgvYOEW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6424d2c4-419f-4c50-8cad-d75bec7923fa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================\n",
        "# STEP 3: Loss & Optimizer\n",
        "# ===================\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(alexnet.parameters(), lr=config.learning_rate)"
      ],
      "metadata": {
        "id": "mxiVbXfxYPiW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_g-opvZQESck"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        running_loss = 0.0\n",
        "\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            batch_correct = (preds == labels).sum().item()\n",
        "            train_correct += batch_correct\n",
        "            train_total += labels.size(0)\n",
        "\n",
        "            # Print every 10 batches\n",
        "            if (i + 1) % 10 == 0:\n",
        "                batch_acc = batch_correct / labels.size(0)\n",
        "                print(f\"[Batch {i+1}/{len(train_loader)}] Loss: {loss.item():.4f}, Batch Acc: {batch_acc:.4f}\")\n",
        "\n",
        "        train_acc = train_correct / train_total\n",
        "        wandb.log({\"epoch\": epoch + 1, \"train_loss\": running_loss, \"train_accuracy\": train_acc})\n",
        "        print(f\"Epoch {epoch+1} Summary - Loss: {running_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        val_acc = val_correct / val_total\n",
        "        wandb.log({\"epoch\": epoch + 1, \"val_accuracy\": val_acc})\n",
        "        print(f\"Validation Accuracy: {val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================\n",
        "# Train the model\n",
        "# ===================\n",
        "train_model(alexnet, criterion, optimizer, train_loader, val_loader, epochs=config.epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTczSawNYSrI",
        "outputId": "41eae913-d2d6-4dd8-f08e-a94e35919168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.8007, Batch Acc: 0.5625\n",
            "[Batch 20/251] Loss: 0.6390, Batch Acc: 0.6250\n",
            "[Batch 30/251] Loss: 0.7546, Batch Acc: 0.6875\n",
            "[Batch 40/251] Loss: 0.6132, Batch Acc: 0.7500\n",
            "[Batch 50/251] Loss: 0.5645, Batch Acc: 0.7500\n",
            "[Batch 60/251] Loss: 0.7108, Batch Acc: 0.6250\n",
            "[Batch 70/251] Loss: 0.9244, Batch Acc: 0.5625\n",
            "[Batch 80/251] Loss: 0.3501, Batch Acc: 0.8750\n",
            "[Batch 90/251] Loss: 0.5794, Batch Acc: 0.6875\n",
            "[Batch 100/251] Loss: 0.4885, Batch Acc: 0.8125\n",
            "[Batch 110/251] Loss: 0.5081, Batch Acc: 0.8125\n",
            "[Batch 120/251] Loss: 0.8510, Batch Acc: 0.6875\n",
            "[Batch 130/251] Loss: 0.5772, Batch Acc: 0.6875\n",
            "[Batch 140/251] Loss: 0.4600, Batch Acc: 0.8750\n",
            "[Batch 150/251] Loss: 0.5847, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.4172, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.4912, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.1670, Batch Acc: 0.9375\n",
            "[Batch 190/251] Loss: 0.7782, Batch Acc: 0.7500\n",
            "[Batch 200/251] Loss: 1.3613, Batch Acc: 0.6250\n",
            "[Batch 210/251] Loss: 0.7417, Batch Acc: 0.6875\n",
            "[Batch 220/251] Loss: 0.8161, Batch Acc: 0.6875\n",
            "[Batch 230/251] Loss: 0.5822, Batch Acc: 0.6875\n",
            "[Batch 240/251] Loss: 0.5377, Batch Acc: 0.8125\n",
            "[Batch 250/251] Loss: 0.5717, Batch Acc: 0.7500\n",
            "Epoch 1 Summary - Loss: 192.5329, Train Accuracy: 0.7363\n",
            "Validation Accuracy: 0.8289\n",
            "\n",
            "Epoch 2/20\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.5284, Batch Acc: 0.8125\n",
            "[Batch 20/251] Loss: 0.1683, Batch Acc: 0.8750\n",
            "[Batch 30/251] Loss: 0.3984, Batch Acc: 0.8125\n",
            "[Batch 40/251] Loss: 0.3464, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.5033, Batch Acc: 0.8125\n",
            "[Batch 60/251] Loss: 0.4091, Batch Acc: 0.8125\n",
            "[Batch 70/251] Loss: 0.4000, Batch Acc: 0.8125\n",
            "[Batch 80/251] Loss: 0.3198, Batch Acc: 0.7500\n",
            "[Batch 90/251] Loss: 0.2934, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.8750, Batch Acc: 0.7500\n",
            "[Batch 110/251] Loss: 0.3530, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.4334, Batch Acc: 0.8750\n",
            "[Batch 130/251] Loss: 0.7208, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.4383, Batch Acc: 0.7500\n",
            "[Batch 150/251] Loss: 0.3665, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.2211, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.7826, Batch Acc: 0.8125\n",
            "[Batch 180/251] Loss: 0.0771, Batch Acc: 1.0000\n",
            "[Batch 190/251] Loss: 0.1774, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.7763, Batch Acc: 0.8125\n",
            "[Batch 210/251] Loss: 0.2608, Batch Acc: 0.8125\n",
            "[Batch 220/251] Loss: 0.3473, Batch Acc: 0.8125\n",
            "[Batch 230/251] Loss: 0.3451, Batch Acc: 0.8125\n",
            "[Batch 240/251] Loss: 0.0456, Batch Acc: 1.0000\n",
            "[Batch 250/251] Loss: 0.2559, Batch Acc: 0.8750\n",
            "Epoch 2 Summary - Loss: 113.0021, Train Accuracy: 0.8396\n",
            "Validation Accuracy: 0.8408\n",
            "\n",
            "Epoch 3/20\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.1080, Batch Acc: 1.0000\n",
            "[Batch 20/251] Loss: 0.4841, Batch Acc: 0.8750\n",
            "[Batch 30/251] Loss: 0.2849, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.8865, Batch Acc: 0.6875\n",
            "[Batch 50/251] Loss: 0.2250, Batch Acc: 0.9375\n",
            "[Batch 60/251] Loss: 0.6805, Batch Acc: 0.8750\n",
            "[Batch 70/251] Loss: 0.2264, Batch Acc: 0.9375\n",
            "[Batch 80/251] Loss: 0.2298, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.0645, Batch Acc: 1.0000\n",
            "[Batch 100/251] Loss: 0.2677, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.2552, Batch Acc: 0.9375\n",
            "[Batch 120/251] Loss: 0.0236, Batch Acc: 1.0000\n",
            "[Batch 130/251] Loss: 0.6615, Batch Acc: 0.7500\n",
            "[Batch 140/251] Loss: 0.4444, Batch Acc: 0.8125\n",
            "[Batch 150/251] Loss: 0.0750, Batch Acc: 1.0000\n",
            "[Batch 160/251] Loss: 0.4859, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.6050, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.0701, Batch Acc: 1.0000\n",
            "[Batch 190/251] Loss: 0.1174, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.2103, Batch Acc: 0.8750\n",
            "[Batch 210/251] Loss: 0.3228, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.3066, Batch Acc: 0.8125\n",
            "[Batch 230/251] Loss: 0.8868, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.2344, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.9556, Batch Acc: 0.6875\n",
            "Epoch 3 Summary - Loss: 91.4558, Train Accuracy: 0.8772\n",
            "Validation Accuracy: 0.8576\n",
            "\n",
            "Epoch 4/20\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.5478, Batch Acc: 0.8125\n",
            "[Batch 20/251] Loss: 0.4903, Batch Acc: 0.8750\n",
            "[Batch 30/251] Loss: 0.3439, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.5322, Batch Acc: 0.8125\n",
            "[Batch 50/251] Loss: 0.0606, Batch Acc: 1.0000\n",
            "[Batch 60/251] Loss: 0.0611, Batch Acc: 1.0000\n",
            "[Batch 70/251] Loss: 0.2593, Batch Acc: 0.9375\n",
            "[Batch 80/251] Loss: 0.6515, Batch Acc: 0.6875\n",
            "[Batch 90/251] Loss: 0.1224, Batch Acc: 0.9375\n",
            "[Batch 100/251] Loss: 0.3810, Batch Acc: 0.7500\n",
            "[Batch 110/251] Loss: 0.2865, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.5360, Batch Acc: 0.8125\n",
            "[Batch 130/251] Loss: 0.3443, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.1086, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.2973, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.2799, Batch Acc: 0.8125\n",
            "[Batch 170/251] Loss: 0.1323, Batch Acc: 0.9375\n",
            "[Batch 180/251] Loss: 0.3864, Batch Acc: 0.8125\n",
            "[Batch 190/251] Loss: 0.2242, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.6093, Batch Acc: 0.7500\n",
            "[Batch 210/251] Loss: 0.6095, Batch Acc: 0.8125\n",
            "[Batch 220/251] Loss: 0.0358, Batch Acc: 1.0000\n",
            "[Batch 230/251] Loss: 0.3046, Batch Acc: 0.9375\n",
            "[Batch 240/251] Loss: 0.0236, Batch Acc: 1.0000\n",
            "[Batch 250/251] Loss: 0.3972, Batch Acc: 0.7500\n",
            "Epoch 4 Summary - Loss: 79.1827, Train Accuracy: 0.8865\n",
            "Validation Accuracy: 0.8536\n",
            "\n",
            "Epoch 5/20\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.2864, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.7903, Batch Acc: 0.8125\n",
            "[Batch 30/251] Loss: 0.1485, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.0279, Batch Acc: 1.0000\n",
            "[Batch 50/251] Loss: 0.1177, Batch Acc: 1.0000\n",
            "[Batch 60/251] Loss: 0.2143, Batch Acc: 0.8750\n",
            "[Batch 70/251] Loss: 0.1693, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.1920, Batch Acc: 0.8750\n",
            "[Batch 90/251] Loss: 0.0289, Batch Acc: 1.0000\n",
            "[Batch 100/251] Loss: 0.0458, Batch Acc: 1.0000\n",
            "[Batch 110/251] Loss: 0.5671, Batch Acc: 0.6875\n",
            "[Batch 120/251] Loss: 0.2649, Batch Acc: 0.8750\n",
            "[Batch 130/251] Loss: 0.5096, Batch Acc: 0.9375\n",
            "[Batch 140/251] Loss: 0.0316, Batch Acc: 1.0000\n",
            "[Batch 150/251] Loss: 0.2881, Batch Acc: 0.8125\n",
            "[Batch 160/251] Loss: 0.3733, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.6457, Batch Acc: 0.8125\n",
            "[Batch 180/251] Loss: 0.2620, Batch Acc: 0.8750\n",
            "[Batch 190/251] Loss: 0.2732, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.0382, Batch Acc: 1.0000\n",
            "[Batch 210/251] Loss: 0.0098, Batch Acc: 1.0000\n",
            "[Batch 220/251] Loss: 0.8931, Batch Acc: 0.6875\n",
            "[Batch 230/251] Loss: 0.0239, Batch Acc: 1.0000\n",
            "[Batch 240/251] Loss: 0.1942, Batch Acc: 0.9375\n",
            "[Batch 250/251] Loss: 0.0707, Batch Acc: 1.0000\n",
            "Epoch 5 Summary - Loss: 74.2272, Train Accuracy: 0.8990\n",
            "Validation Accuracy: 0.8882\n",
            "\n",
            "Epoch 6/20\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.0284, Batch Acc: 1.0000\n",
            "[Batch 20/251] Loss: 0.0497, Batch Acc: 1.0000\n",
            "[Batch 30/251] Loss: 0.0768, Batch Acc: 1.0000\n",
            "[Batch 40/251] Loss: 0.0296, Batch Acc: 1.0000\n",
            "[Batch 50/251] Loss: 0.1126, Batch Acc: 0.9375\n",
            "[Batch 60/251] Loss: 0.2204, Batch Acc: 0.9375\n",
            "[Batch 70/251] Loss: 0.4328, Batch Acc: 0.7500\n",
            "[Batch 80/251] Loss: 0.0948, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.4523, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.4622, Batch Acc: 0.8125\n",
            "[Batch 110/251] Loss: 0.0887, Batch Acc: 1.0000\n",
            "[Batch 120/251] Loss: 0.3828, Batch Acc: 0.8750\n",
            "[Batch 130/251] Loss: 0.1526, Batch Acc: 0.9375\n",
            "[Batch 140/251] Loss: 0.7312, Batch Acc: 0.7500\n",
            "[Batch 150/251] Loss: 0.0804, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.8827, Batch Acc: 0.8750\n",
            "[Batch 170/251] Loss: 0.1396, Batch Acc: 0.9375\n",
            "[Batch 180/251] Loss: 0.3371, Batch Acc: 0.8750\n",
            "[Batch 190/251] Loss: 0.4769, Batch Acc: 0.8125\n",
            "[Batch 200/251] Loss: 1.0014, Batch Acc: 0.8125\n",
            "[Batch 210/251] Loss: 0.4530, Batch Acc: 0.9375\n",
            "[Batch 220/251] Loss: 0.1411, Batch Acc: 0.9375\n",
            "[Batch 230/251] Loss: 0.6029, Batch Acc: 0.7500\n",
            "[Batch 240/251] Loss: 0.5324, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.2467, Batch Acc: 0.9375\n",
            "Epoch 6 Summary - Loss: 74.4835, Train Accuracy: 0.8972\n",
            "Validation Accuracy: 0.8467\n",
            "\n",
            "Epoch 7/20\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.1982, Batch Acc: 0.8750\n",
            "[Batch 20/251] Loss: 0.1527, Batch Acc: 0.9375\n",
            "[Batch 30/251] Loss: 0.4863, Batch Acc: 0.8125\n",
            "[Batch 40/251] Loss: 0.5000, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.0527, Batch Acc: 1.0000\n",
            "[Batch 60/251] Loss: 0.1698, Batch Acc: 0.9375\n",
            "[Batch 70/251] Loss: 0.5836, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.5453, Batch Acc: 0.8125\n",
            "[Batch 90/251] Loss: 0.1855, Batch Acc: 0.9375\n",
            "[Batch 100/251] Loss: 0.7658, Batch Acc: 0.7500\n",
            "[Batch 110/251] Loss: 0.0265, Batch Acc: 1.0000\n",
            "[Batch 120/251] Loss: 0.5193, Batch Acc: 0.8750\n",
            "[Batch 130/251] Loss: 0.3792, Batch Acc: 0.8125\n",
            "[Batch 140/251] Loss: 0.1753, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.0886, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.3758, Batch Acc: 0.8750\n",
            "[Batch 170/251] Loss: 0.8462, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.4403, Batch Acc: 0.8125\n",
            "[Batch 190/251] Loss: 0.3342, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.0510, Batch Acc: 0.9375\n",
            "[Batch 210/251] Loss: 0.1924, Batch Acc: 0.9375\n",
            "[Batch 220/251] Loss: 0.0316, Batch Acc: 1.0000\n",
            "[Batch 230/251] Loss: 0.5667, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.3518, Batch Acc: 0.8125\n",
            "[Batch 250/251] Loss: 0.1795, Batch Acc: 0.9375\n",
            "Epoch 7 Summary - Loss: 67.5862, Train Accuracy: 0.9082\n",
            "Validation Accuracy: 0.8902\n",
            "\n",
            "Epoch 8/20\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.0613, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.0381, Batch Acc: 1.0000\n",
            "[Batch 30/251] Loss: 0.2371, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.1923, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.0690, Batch Acc: 1.0000\n",
            "[Batch 60/251] Loss: 0.0873, Batch Acc: 0.9375\n",
            "[Batch 70/251] Loss: 0.2185, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.2999, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.0583, Batch Acc: 1.0000\n",
            "[Batch 100/251] Loss: 0.0066, Batch Acc: 1.0000\n",
            "[Batch 110/251] Loss: 0.1830, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.3253, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.5601, Batch Acc: 0.8125\n",
            "[Batch 140/251] Loss: 0.1882, Batch Acc: 0.8125\n",
            "[Batch 150/251] Loss: 0.0089, Batch Acc: 1.0000\n",
            "[Batch 160/251] Loss: 0.1520, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.1961, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.1242, Batch Acc: 1.0000\n",
            "[Batch 190/251] Loss: 0.2468, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.2385, Batch Acc: 0.8750\n",
            "[Batch 210/251] Loss: 0.0202, Batch Acc: 1.0000\n",
            "[Batch 220/251] Loss: 0.4746, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 0.3235, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.3160, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.2063, Batch Acc: 0.9375\n",
            "Epoch 8 Summary - Loss: 59.2637, Train Accuracy: 0.9177\n",
            "Validation Accuracy: 0.8823\n",
            "\n",
            "Epoch 9/20\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.0026, Batch Acc: 1.0000\n",
            "[Batch 20/251] Loss: 0.1752, Batch Acc: 0.9375\n",
            "[Batch 30/251] Loss: 0.5152, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.0043, Batch Acc: 1.0000\n",
            "[Batch 50/251] Loss: 0.3015, Batch Acc: 0.9375\n",
            "[Batch 60/251] Loss: 0.0712, Batch Acc: 1.0000\n",
            "[Batch 70/251] Loss: 0.1401, Batch Acc: 0.9375\n",
            "[Batch 80/251] Loss: 0.1757, Batch Acc: 0.8750\n",
            "[Batch 90/251] Loss: 0.8470, Batch Acc: 0.8125\n",
            "[Batch 100/251] Loss: 0.4067, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.5877, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.3683, Batch Acc: 0.8125\n",
            "[Batch 130/251] Loss: 0.0824, Batch Acc: 1.0000\n",
            "[Batch 140/251] Loss: 0.3115, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.1925, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.9028, Batch Acc: 0.7500\n",
            "[Batch 170/251] Loss: 0.8998, Batch Acc: 0.7500\n",
            "[Batch 180/251] Loss: 0.4778, Batch Acc: 0.8125\n",
            "[Batch 190/251] Loss: 0.4836, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 1.1132, Batch Acc: 0.6875\n",
            "[Batch 210/251] Loss: 0.0197, Batch Acc: 1.0000\n",
            "[Batch 220/251] Loss: 0.2907, Batch Acc: 0.9375\n",
            "[Batch 230/251] Loss: 0.1611, Batch Acc: 0.9375\n",
            "[Batch 240/251] Loss: 0.1158, Batch Acc: 0.9375\n",
            "[Batch 250/251] Loss: 0.2222, Batch Acc: 0.9375\n",
            "Epoch 9 Summary - Loss: 65.6981, Train Accuracy: 0.9159\n",
            "Validation Accuracy: 0.8912\n",
            "\n",
            "Epoch 10/20\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.0178, Batch Acc: 1.0000\n",
            "[Batch 20/251] Loss: 0.0587, Batch Acc: 1.0000\n",
            "[Batch 30/251] Loss: 0.3842, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.2438, Batch Acc: 0.9375\n",
            "[Batch 50/251] Loss: 0.4940, Batch Acc: 0.8750\n",
            "[Batch 60/251] Loss: 0.6596, Batch Acc: 0.9375\n",
            "[Batch 70/251] Loss: 0.4885, Batch Acc: 0.8125\n",
            "[Batch 80/251] Loss: 0.2209, Batch Acc: 0.8125\n",
            "[Batch 90/251] Loss: 0.1657, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.0921, Batch Acc: 1.0000\n",
            "[Batch 110/251] Loss: 0.5133, Batch Acc: 0.9375\n",
            "[Batch 120/251] Loss: 0.3298, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.4933, Batch Acc: 0.8125\n",
            "[Batch 140/251] Loss: 0.2932, Batch Acc: 0.8750\n",
            "[Batch 150/251] Loss: 0.0394, Batch Acc: 1.0000\n",
            "[Batch 160/251] Loss: 0.0922, Batch Acc: 1.0000\n",
            "[Batch 170/251] Loss: 0.3204, Batch Acc: 0.9375\n",
            "[Batch 180/251] Loss: 0.2600, Batch Acc: 0.8750\n",
            "[Batch 190/251] Loss: 0.1662, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.1310, Batch Acc: 0.9375\n",
            "[Batch 210/251] Loss: 0.0197, Batch Acc: 1.0000\n",
            "[Batch 220/251] Loss: 0.6181, Batch Acc: 0.9375\n",
            "[Batch 230/251] Loss: 0.3593, Batch Acc: 0.8125\n",
            "[Batch 240/251] Loss: 0.0538, Batch Acc: 1.0000\n",
            "[Batch 250/251] Loss: 0.1750, Batch Acc: 0.8750\n",
            "Epoch 10 Summary - Loss: 59.9995, Train Accuracy: 0.9229\n",
            "Validation Accuracy: 0.8783\n",
            "\n",
            "Epoch 11/20\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.0101, Batch Acc: 1.0000\n",
            "[Batch 20/251] Loss: 0.0671, Batch Acc: 0.9375\n",
            "[Batch 30/251] Loss: 0.1286, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.3818, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.1090, Batch Acc: 0.9375\n",
            "[Batch 60/251] Loss: 0.1268, Batch Acc: 0.9375\n",
            "[Batch 70/251] Loss: 0.1189, Batch Acc: 0.9375\n",
            "[Batch 80/251] Loss: 0.0096, Batch Acc: 1.0000\n",
            "[Batch 90/251] Loss: 0.0259, Batch Acc: 1.0000\n",
            "[Batch 100/251] Loss: 0.2659, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.4541, Batch Acc: 0.9375\n",
            "[Batch 120/251] Loss: 0.0422, Batch Acc: 1.0000\n",
            "[Batch 130/251] Loss: 0.8755, Batch Acc: 0.8125\n",
            "[Batch 140/251] Loss: 0.2144, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.1821, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.3061, Batch Acc: 0.8125\n",
            "[Batch 170/251] Loss: 0.7482, Batch Acc: 0.8125\n",
            "[Batch 180/251] Loss: 0.5507, Batch Acc: 0.8750\n",
            "[Batch 190/251] Loss: 0.5995, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.0037, Batch Acc: 1.0000\n",
            "[Batch 210/251] Loss: 0.4850, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.1522, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 0.0201, Batch Acc: 1.0000\n",
            "[Batch 240/251] Loss: 0.0457, Batch Acc: 1.0000\n",
            "[Batch 250/251] Loss: 0.7903, Batch Acc: 0.8750\n",
            "Epoch 11 Summary - Loss: 61.7620, Train Accuracy: 0.9192\n",
            "Validation Accuracy: 0.9021\n",
            "\n",
            "Epoch 12/20\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.0062, Batch Acc: 1.0000\n",
            "[Batch 20/251] Loss: 0.0529, Batch Acc: 0.9375\n",
            "[Batch 30/251] Loss: 0.1429, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.1564, Batch Acc: 0.9375\n",
            "[Batch 50/251] Loss: 0.0832, Batch Acc: 0.9375\n",
            "[Batch 60/251] Loss: 0.0350, Batch Acc: 1.0000\n",
            "[Batch 70/251] Loss: 0.3185, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.1516, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.0141, Batch Acc: 1.0000\n",
            "[Batch 100/251] Loss: 0.3432, Batch Acc: 0.9375\n",
            "[Batch 110/251] Loss: 0.1232, Batch Acc: 0.9375\n",
            "[Batch 120/251] Loss: 0.3300, Batch Acc: 0.8750\n",
            "[Batch 130/251] Loss: 0.0666, Batch Acc: 1.0000\n",
            "[Batch 140/251] Loss: 0.4501, Batch Acc: 0.8750\n",
            "[Batch 150/251] Loss: 0.2590, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.0213, Batch Acc: 1.0000\n",
            "[Batch 170/251] Loss: 0.1438, Batch Acc: 0.9375\n",
            "[Batch 180/251] Loss: 0.1578, Batch Acc: 0.8750\n",
            "[Batch 190/251] Loss: 0.0287, Batch Acc: 1.0000\n",
            "[Batch 200/251] Loss: 0.0223, Batch Acc: 1.0000\n",
            "[Batch 210/251] Loss: 0.0407, Batch Acc: 1.0000\n",
            "[Batch 220/251] Loss: 0.3588, Batch Acc: 0.8125\n",
            "[Batch 230/251] Loss: 0.0663, Batch Acc: 0.9375\n",
            "[Batch 240/251] Loss: 0.3886, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.3305, Batch Acc: 0.9375\n",
            "Epoch 12 Summary - Loss: 53.0684, Train Accuracy: 0.9314\n",
            "Validation Accuracy: 0.9031\n",
            "\n",
            "Epoch 13/20\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.1592, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.0083, Batch Acc: 1.0000\n",
            "[Batch 30/251] Loss: 0.1144, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.3147, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.2098, Batch Acc: 0.9375\n",
            "[Batch 60/251] Loss: 0.0978, Batch Acc: 0.9375\n",
            "[Batch 70/251] Loss: 0.5806, Batch Acc: 0.9375\n",
            "[Batch 80/251] Loss: 0.0082, Batch Acc: 1.0000\n",
            "[Batch 90/251] Loss: 0.1343, Batch Acc: 0.9375\n",
            "[Batch 100/251] Loss: 0.0891, Batch Acc: 0.9375\n",
            "[Batch 110/251] Loss: 0.1459, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.0089, Batch Acc: 1.0000\n",
            "[Batch 130/251] Loss: 0.0199, Batch Acc: 1.0000\n",
            "[Batch 140/251] Loss: 0.2715, Batch Acc: 0.8750\n",
            "[Batch 150/251] Loss: 0.6992, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.4132, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 1.7092, Batch Acc: 0.8125\n",
            "[Batch 180/251] Loss: 0.1053, Batch Acc: 0.9375\n",
            "[Batch 190/251] Loss: 0.5612, Batch Acc: 0.8125\n",
            "[Batch 200/251] Loss: 0.1509, Batch Acc: 0.9375\n",
            "[Batch 210/251] Loss: 0.1993, Batch Acc: 0.9375\n",
            "[Batch 220/251] Loss: 0.0531, Batch Acc: 1.0000\n",
            "[Batch 230/251] Loss: 0.0752, Batch Acc: 0.9375\n",
            "[Batch 240/251] Loss: 0.1319, Batch Acc: 0.9375\n",
            "[Batch 250/251] Loss: 0.1830, Batch Acc: 0.8750\n",
            "Epoch 13 Summary - Loss: 58.2695, Train Accuracy: 0.9247\n",
            "Validation Accuracy: 0.8932\n",
            "\n",
            "Epoch 14/20\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.0214, Batch Acc: 1.0000\n",
            "[Batch 20/251] Loss: 0.0614, Batch Acc: 0.9375\n",
            "[Batch 30/251] Loss: 0.3453, Batch Acc: 0.8125\n",
            "[Batch 40/251] Loss: 0.0208, Batch Acc: 1.0000\n",
            "[Batch 50/251] Loss: 0.2633, Batch Acc: 0.9375\n",
            "[Batch 60/251] Loss: 0.3256, Batch Acc: 0.8750\n",
            "[Batch 70/251] Loss: 0.0254, Batch Acc: 1.0000\n",
            "[Batch 80/251] Loss: 0.5619, Batch Acc: 0.8750\n",
            "[Batch 90/251] Loss: 0.3836, Batch Acc: 0.9375\n",
            "[Batch 100/251] Loss: 0.0297, Batch Acc: 1.0000\n",
            "[Batch 110/251] Loss: 0.0322, Batch Acc: 1.0000\n",
            "[Batch 120/251] Loss: 0.3185, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.0478, Batch Acc: 1.0000\n",
            "[Batch 140/251] Loss: 0.0793, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.2393, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.4573, Batch Acc: 0.8750\n",
            "[Batch 170/251] Loss: 0.5157, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.0358, Batch Acc: 1.0000\n",
            "[Batch 190/251] Loss: 0.0493, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.1797, Batch Acc: 0.9375\n",
            "[Batch 210/251] Loss: 0.4331, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.4901, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 0.0554, Batch Acc: 1.0000\n",
            "[Batch 240/251] Loss: 0.0294, Batch Acc: 1.0000\n",
            "[Batch 250/251] Loss: 0.0679, Batch Acc: 1.0000\n",
            "Epoch 14 Summary - Loss: 56.9802, Train Accuracy: 0.9251\n",
            "Validation Accuracy: 0.9110\n",
            "\n",
            "Epoch 15/20\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.1688, Batch Acc: 0.8750\n",
            "[Batch 20/251] Loss: 0.1179, Batch Acc: 0.9375\n",
            "[Batch 30/251] Loss: 0.2969, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.0091, Batch Acc: 1.0000\n",
            "[Batch 50/251] Loss: 0.2356, Batch Acc: 0.8750\n",
            "[Batch 60/251] Loss: 0.1993, Batch Acc: 0.8750\n",
            "[Batch 70/251] Loss: 0.0025, Batch Acc: 1.0000\n",
            "[Batch 80/251] Loss: 0.3082, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.4657, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.0450, Batch Acc: 0.9375\n",
            "[Batch 110/251] Loss: 0.0269, Batch Acc: 1.0000\n",
            "[Batch 120/251] Loss: 0.0067, Batch Acc: 1.0000\n",
            "[Batch 130/251] Loss: 0.1752, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.2449, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.0435, Batch Acc: 1.0000\n",
            "[Batch 160/251] Loss: 0.0114, Batch Acc: 1.0000\n",
            "[Batch 170/251] Loss: 0.0094, Batch Acc: 1.0000\n",
            "[Batch 180/251] Loss: 0.2212, Batch Acc: 0.9375\n",
            "[Batch 190/251] Loss: 0.1671, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.0091, Batch Acc: 1.0000\n",
            "[Batch 210/251] Loss: 0.0379, Batch Acc: 1.0000\n",
            "[Batch 220/251] Loss: 0.2733, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 1.1955, Batch Acc: 0.6875\n",
            "[Batch 240/251] Loss: 0.0217, Batch Acc: 1.0000\n",
            "[Batch 250/251] Loss: 0.5521, Batch Acc: 0.9375\n",
            "Epoch 15 Summary - Loss: 60.1378, Train Accuracy: 0.9229\n",
            "Validation Accuracy: 0.9070\n",
            "\n",
            "Epoch 16/20\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.1983, Batch Acc: 0.8750\n",
            "[Batch 20/251] Loss: 0.1279, Batch Acc: 0.9375\n",
            "[Batch 30/251] Loss: 0.0910, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.1188, Batch Acc: 0.9375\n",
            "[Batch 50/251] Loss: 0.0119, Batch Acc: 1.0000\n",
            "[Batch 60/251] Loss: 0.2440, Batch Acc: 0.8750\n",
            "[Batch 70/251] Loss: 0.0336, Batch Acc: 1.0000\n",
            "[Batch 80/251] Loss: 0.0788, Batch Acc: 1.0000\n",
            "[Batch 90/251] Loss: 0.6516, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.6153, Batch Acc: 0.8125\n",
            "[Batch 110/251] Loss: 0.2099, Batch Acc: 0.9375\n",
            "[Batch 120/251] Loss: 0.1098, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.1279, Batch Acc: 0.9375\n",
            "[Batch 140/251] Loss: 0.0199, Batch Acc: 1.0000\n",
            "[Batch 150/251] Loss: 0.0944, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.4541, Batch Acc: 0.8125\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}